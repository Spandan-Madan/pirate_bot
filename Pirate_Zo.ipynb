{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using /tmp/tfhub_modules to cache modules.\n",
      "INFO:tensorflow:Downloading TF-Hub Module 'https://tfhub.dev/google/universal-sentence-encoder-lite/2'.\n",
      "INFO:tensorflow:Downloaded TF-Hub Module 'https://tfhub.dev/google/universal-sentence-encoder-lite/2'.\n",
      "SentencePiece model loaded at b'/tmp/tfhub_modules/539544f0a997d91c327c23285ea00c37588d92cc/assets/universal_encoder_8k_spm.model'.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/usr/local/lib/python3.5/dist-packages/')\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import os\n",
    "import http.client, urllib.request, urllib.parse, urllib.error, base64\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pickle\n",
    "import sentencepiece as spm\n",
    "\n",
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder-lite/2\"\n",
    "embed = hub.Module(module_url)\n",
    "tf.logging.set_verbosity(tf.logging.WARN)\n",
    "\n",
    "module = hub.Module(\"https://tfhub.dev/google/universal-sentence-encoder-lite/2\")\n",
    "input_placeholder = tf.sparse_placeholder(tf.int64, shape=[None, None])\n",
    "encodings = module(\n",
    "    inputs=dict(\n",
    "        values=input_placeholder.values,\n",
    "        indices=input_placeholder.indices,\n",
    "        dense_shape=input_placeholder.dense_shape))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    spm_path = sess.run(module(signature=\"spm_path\"))\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load(spm_path)\n",
    "print(\"SentencePiece model loaded at {}.\".format(spm_path))\n",
    "\n",
    "def process_to_IDs_in_sparse_format(sp, sentences):\n",
    "  # An utility method that processes sentences with the sentence piece processor\n",
    "  # 'sp' and returns the results in tf.SparseTensor-similar format:\n",
    "  # (values, indices, dense_shape)\n",
    "    ids = [sp.EncodeAsIds(x) for x in sentences]\n",
    "    max_len = max(len(x) for x in ids)\n",
    "    dense_shape=(len(ids), max_len)\n",
    "    values=[item for sublist in ids for item in sublist]\n",
    "    indices=[[row,col] for row in range(len(ids)) for col in range(len(ids[row]))]\n",
    "    return (values, indices, dense_shape)\n",
    "\n",
    "def embed_sentence_lite(sentences):\n",
    "    messages = sentences\n",
    "    values, indices, dense_shape = process_to_IDs_in_sparse_format(sp, messages)\n",
    "\n",
    "    # Reduce logging output.\n",
    "    tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "    with tf.Session() as session:\n",
    "        session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "        message_embeddings = session.run(\n",
    "          encodings,\n",
    "          feed_dict={input_placeholder.values: values,\n",
    "                    input_placeholder.indices: indices,\n",
    "                    input_placeholder.dense_shape: dense_shape})\n",
    "    \n",
    "    return message_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../MSR/pirate_bot/data/movie_line_pairs.txt','r') as F:\n",
    "    lines = F.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'line\\tresponse\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "utterances = []\n",
    "utter_to_response = {}\n",
    "for line in lines[1:]:\n",
    "    line = line.rstrip()\n",
    "    p1,p2 = line.split('\\t')\n",
    "    utterances.append(p1)\n",
    "    utter_to_response[p1] = p2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(utterances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest(sentence_rep,query_rep,K):\n",
    "    top_K = np.argsort(np.sqrt((np.sum(np.square(sentence_rep - query_rep),axis=1))))[:K]\n",
    "    return top_K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "220000\n",
      "230000\n",
      "240000\n",
      "250000\n",
      "260000\n",
      "270000\n",
      "280000\n",
      "290000\n",
      "300000\n"
     ]
    }
   ],
   "source": [
    "all_dataset__embeddings = np.zeros((len(utterances),512))\n",
    "for i in range(0,len(utterances),10000):\n",
    "    print(i)\n",
    "    start = i\n",
    "    end = min(i+100,len(utterances))\n",
    "    chunk = utterances[start:end]\n",
    "    chunk_embedding = embed_sentence_lite(chunk)\n",
    "    all_dataset__embeddings[start:end] = chunk_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('../MSR/pirate_bot/data/movie_line_embeddings.p','wb') as F:\n",
    "    pickle.dump(all_dataset__embeddings,F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# f = open('res/other_embeddings.p','rb')\n",
    "# other_embeddings = pickle.load(f)\n",
    "# f.close()\n",
    "\n",
    "# f = open('res/your_embeddings.p','rb')\n",
    "# your_embeddings = pickle.load(f)\n",
    "# f.close()\n",
    "\n",
    "# f = open('res/dilogues.p','rb')\n",
    "# pr_to_sp = pickle.load(f)\n",
    "# f.close()\n",
    "\n",
    "\n",
    "# f = open('res/your_sents.p','rb')\n",
    "# your_sentences = pickle.load(f)\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keys = list(pr_to_sp.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = open('res/key_embeddings.p','rb')\n",
    "# key_embeddings = pickle.load(f)\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def speak_like_me(query,K,your_embeddings,other_embeddings,your_sen):\n",
    "#     other_query = [query]\n",
    "#     query_embedding = embed_sentence_lite(other_query)\n",
    "#     closest_your = find_closest(your_embeddings,query_embedding,K)\n",
    "#     for cl in closest_your:\n",
    "#         print(your_sentences[cl])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def respond_like_me(query,K,key_embeddings):\n",
    "    other_query = [query]\n",
    "    query_embedding = embed_sentence_lite(other_query)\n",
    "    closest_other = find_closest(key_embeddings,query_embedding,K+2)\n",
    "    return closest_other\n",
    "#     for k in closest_other[3:]:\n",
    "#         print([keys[k]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = respond_like_me(\"ho ho ho and a bottle of rum\",10,all_dataset__embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's have a go at the rum.\n",
      "Hey you boys, take the bottle with you.\n",
      "Here's the key to the rum store.\n",
      "That's the drink.\n",
      "That's what drink does to you.\n",
      "Rum-looking lot, eh, sir?\n",
      "We call it rum.\n",
      "A bit of water!\n",
      "I'm not drunk enough to go on a boat.\n",
      "Land ho!\n",
      "Land ho!\n",
      "Water casks ready for filling, sir.\n"
     ]
    }
   ],
   "source": [
    "t = []\n",
    "for i in k:\n",
    "    print(utterances[i])\n",
    "    t.append(utter_to_response[utterances[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Yeah.',\n",
       " 'I noticed he was pale, take him outside!',\n",
       " \"That's what drink does to you.\",\n",
       " 'Would that do?',\n",
       " \"I'll never touch another drop of drink as long as I live.\",\n",
       " 'Yes, you are.',\n",
       " 'I suppose I have you to thank.',\n",
       " 'Where away?',\n",
       " 'The ship cleared for action?',\n",
       " 'You fill a pannikin and bring it back.']"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(t,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_tens",
   "language": "python",
   "name": "torch_tens"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
